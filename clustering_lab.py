# -*- coding: utf-8 -*-
"""Clustering lab.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rQ3kfDmsjBUihrF5nDtpm7keQgFJdiwA
"""

import numpy as np
import pandas as pd
import seaborn as sns
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA

df = pd.read_csv("/content/winequality.csv")
df.sample(10)

### STATISTICAL SUMMARY (Excluding NaN)
df.describe()
display(df.describe())
### STATISTICAL SUMMARY (Including NaN)
df.describe(include='all')
display(df.describe(include='all'))

""" Noticed that the weren't statistical summary for NaN /n
This may be bcos of NaN doesn't even exist in the dataset to confirm this I decided to check the missing value below
"""

display(df.info())

missing_summary = pd.DataFrame({
    'Missing Exists': df.isna().any(),
    'Missing Count': df.isna().sum()
})
print(missing_summary)

df_clean = df.copy()
for c in df_clean.columns:
    if pd.api.types.is_numeric_dtype(df_clean[c]):
        df_clean[c] = df_clean[c].fillna(df_clean[c].median())
    else:
        df_clean[c] = df_clean[c].fillna(df_clean[c].mode().iloc[0])
df_clean

### Outlier check
def iqr_count(s):
    q1, q3 = s.quantile(0.25), s.quantile(0.75)
    iqr = q3 - q1
    lo, hi = q1 - 1.5*iqr, q3 + 1.5*iqr
    return int(((s < lo) | (s > hi)).sum())

outlier_counts = {c: iqr_count(df_clean[c]) for c in df_clean.select_dtypes("number").columns}
outlier_counts = dict(sorted(outlier_counts.items(), key=lambda x: x[1], reverse=True))
outlier_counts

outlier_summary = (
    df_clean.select_dtypes("number")
           .apply(iqr_count)
           .sort_values(ascending=False)
)
display(outlier_summary)

feature_cols = [c for c in df_clean.columns if c != label_col]
X_raw = df_clean[feature_cols]
X_raw

# Columns with the most outliers based on the previous analysis
cols_with_most_outliers = ['residual sugar', 'density', 'chlorides', 'sulphates']

# Create histograms for each of these columns
for col in cols_with_most_outliers:
    plt.figure(figsize=(8, 4))
    sns.histplot(data=df_clean, x=col, kde=True)
    plt.title(f'Histogram of {col}')
    plt.xlabel(col)
    plt.ylabel('Frequency')
    plt.show()

plt.figure(figsize=(10, 8))
sns.heatmap(df_clean.corr(), annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Heatmap of Wine Quality Data')
plt.show()

from sklearn.preprocessing import StandardScaler

# ---------- 4) Scale ----------
label_col = 'quality' # Replace with your actual target column name if it exists
feature_cols = [c for c in df_clean.columns if c != label_col]
X = df_clean[feature_cols].values
X = StandardScaler().fit_transform(X)

# ---------- 5) PCA (2D for viz) ----------
pca = PCA(n_components=2, random_state=42)
X_pca = pca.fit_transform(X)
pca_var = pca.explained_variance_ratio_
pca_var

# ---------- 6) Tune k (fast, robust) ----------
from sklearn.cluster import KMeans, AgglomerativeClustering
from sklearn.metrics import silhouette_score

# Use a small sample for speed
n = X.shape[0]
sample_n = min(1500, n)
rng = np.random.default_rng(42)
idx = rng.choice(n, size=sample_n, replace=False)
Xs = X[idx]

k_range = list(range(2, 9))
kmeans_sil, kmeans_inertia, agg_sil = [], [], []

for k in k_range:
    km = KMeans(n_clusters=k, n_init=10, random_state=42).fit(Xs)
    kmeans_inertia.append(km.inertia_)
    kmeans_sil.append(silhouette_score(Xs, km.labels_))
    agg = AgglomerativeClustering(n_clusters=k, linkage="ward").fit(Xs)
    agg_sil.append(silhouette_score(Xs, agg.labels_))

best_k_km  = k_range[int(np.argmax(kmeans_sil))]
best_k_agg = k_range[int(np.argmax(agg_sil))]
best_k_agg

# ---------- 7) Final models on full data ----------
labels_km  = KMeans(n_clusters=best_k_km,  n_init=10, random_state=42).fit_predict(X)
labels_agg = AgglomerativeClustering(n_clusters=best_k_agg, linkage="ward").fit_predict(X)

sil_km  = silhouette_score(X, labels_km)
sil_agg = silhouette_score(X, labels_agg)
sil_agg

# ---------- 8) Cluster profiles ----------
def profile(df_full, labels, feats):
    tmp = df_full.copy()
    tmp["_cluster"] = labels
    g = tmp.groupby("_cluster")
    prof = g[feats].mean().round(3)
    prof["count"] = g[feats[0]].count()
    return prof

prof_km  = profile(df_clean, labels_km,  feature_cols)
prof_agg = profile(df_clean, labels_agg, feature_cols)

display(prof_km)

display(prof_agg)

# Visualize the clusters using PCA results
plt.figure(figsize=(8, 6))
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels_km, cmap='viridis', marker='o', s=50, alpha=0.6)
plt.title('KMeans Clustering Results (2D PCA)')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.colorbar(label='Cluster')
plt.grid(True)
plt.show()

import os

# Define the output directory for plots
OUTDIR = 'your_output_directory' # Replace with your desired output directory
os.makedirs(OUTDIR, exist_ok=True) # Create the output directory if it doesn't exist

#### Plots

plt.figure(); plt.plot(k_range, kmeans_inertia, marker="o")
plt.title("Elbow (KMeans inertia vs k)"); plt.xlabel("k"); plt.ylabel("Inertia");
elbow_png = os.path.join(OUTDIR, "elbow.png"); plt.tight_layout(); plt.savefig(elbow_png, dpi=140); plt.close()

plt.figure();
plt.plot(k_range, kmeans_sil, marker="o", label="KMeans")
plt.plot(k_range, agg_sil, marker="o", label="Agglomerative")
plt.title("Silhouette vs k"); plt.xlabel("k"); plt.ylabel("Silhouette"); plt.legend();
sil_png = os.path.join(OUTDIR, "silhouette.png"); plt.tight_layout(); plt.savefig(sil_png, dpi=140); plt.close()

plt.figure(); plt.scatter(X_pca[:,0], X_pca[:,1], c=labels_km)
plt.title(f"PCA scatter – KMeans (k={best_k_km}, silhouette={sil_km:.3f})"); plt.xlabel("PC1"); plt.ylabel("PC2");
km_png = os.path.join(OUTDIR, "pca_kmeans.png"); plt.tight_layout(); plt.savefig(km_png, dpi=140); plt.close()

plt.figure(); plt.scatter(X_pca[:,0], X_pca[:,1], c=labels_agg)
plt.title(f"PCA scatter – Agglomerative (k={best_k_agg}, silhouette={sil_agg:.3f})"); plt.xlabel("PC1"); plt.ylabel("PC2");
agg_png = os.path.join(OUTDIR, "pca_agg.png"); plt.tight_layout(); plt.savefig(agg_png, dpi=140); plt.close()

fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# KMeans Plot
scatter_km = axes[0].scatter(X_pca[:, 0], X_pca[:, 1], c=labels_km, cmap='viridis', marker='o', s=50, alpha=0.6)
axes[0].set_title(f'KMeans Clustering Results (2D PCA, k={best_k_km})')
axes[0].set_xlabel('Principal Component 1')
axes[0].set_ylabel('Principal Component 2')
axes[0].grid(True)
fig.colorbar(scatter_km, ax=axes[0], label='Cluster')


# Agglomerative Clustering Plot
scatter_agg = axes[1].scatter(X_pca[:, 0], X_pca[:, 1], c=labels_agg, cmap='viridis', marker='o', s=50, alpha=0.6)
axes[1].set_title(f'Agglomerative Clustering Results (2D PCA, k={best_k_agg})')
axes[1].set_xlabel('Principal Component 1')
axes[1].set_ylabel('Principal Component 2')
axes[1].grid(True)
fig.colorbar(scatter_agg, ax=axes[1], label='Cluster')


plt.tight_layout()
plt.show()

# Visualize the clusters using PCA results (Agglomerative Clustering)
plt.figure(figsize=(8, 6))
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels_agg, cmap='viridis', marker='o', s=50, alpha=0.6)
plt.title('Agglomerative Clustering Results (2D PCA)')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.colorbar(label='Cluster')
plt.grid(True)
plt.show()



plt.figure(); plt.plot(k_range, kmeans_inertia, marker="o")
plt.title("Elbow (KMeans inertia vs k)"); plt.xlabel("k"); plt.ylabel("Inertia");
plt.tight_layout(); plt.show()

from sklearn.mixture import GaussianMixture
# training ONE gmm at k=best_k_km for reference:
gmm = GaussianMixture(n_components=best_k_km, covariance_type="full", random_state=42).fit(X)
gmm_labels = gmm.predict(X)
gmm_sil = silhouette_score(X, gmm_labels)
print("GMM (k = best_k_km) silhouette:", round(gmm_sil, 3))

# --- JUST HIERARCHICAL (Agglomerative/Ward) ---

import numpy as np
from sklearn.cluster import AgglomerativeClustering
from sklearn.metrics import silhouette_score

# 1) Pick k via silhouette (tune on a sample for speed)
k_range = range(2, 11)
n = X.shape[0]
rng = np.random.default_rng(42)
idx = rng.choice(n, size=min(1500, n), replace=False)   # sample for tuning
Xs = X[idx]

sil_scores = []
for k in k_range:
    labels_s = AgglomerativeClustering(n_clusters=k, linkage="ward").fit_predict(Xs)
    sil_scores.append(silhouette_score(Xs, labels_s))

best_k = list(k_range)[int(np.argmax(sil_scores))]
print(f"Best k (sample): {best_k} | silhouette={max(sil_scores):.3f}")

# 2) Final fit on FULL data
agg = AgglomerativeClustering(n_clusters=best_k, linkage="ward")
labels_agg = agg.fit_predict(X)
sil_full = silhouette_score(X, labels_agg)
print(f"Full-data silhouette (Agglomerative, k={best_k}): {sil_full:.3f}")

# 3) Quick cluster profile (means; post-hoc quality if present)
tmp = df_clean.copy()
tmp["_cluster"] = labels_agg
profile_agg = tmp.groupby("_cluster")[feature_cols].mean().round(3)
profile_agg["count"] = tmp.groupby("_cluster").size()
if "quality" in tmp.columns:
    profile_agg["avg_quality_(posthoc)"] = tmp.groupby("_cluster")["quality"].mean().round(3)

print(profile_agg)

# --- PCA scatter for Agglomerative clusters ---
import numpy as np, pandas as pd, matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import AgglomerativeClustering
from sklearn.metrics import silhouette_score

# Using numeric features for clustering:
feature_cols = df_clean.select_dtypes("number").columns.tolist()
X = StandardScaler().fit_transform(df_clean[feature_cols])

# Using k via silhouette on a sample, then refit on full
k_range = range(2, 11)
rng = np.random.default_rng(42)
idx = rng.choice(X.shape[0], size=min(1500, X.shape[0]), replace=False)
Xs = X[idx]

sil = []
for k in k_range:
    labels_s = AgglomerativeClustering(n_clusters=k, linkage="ward").fit_predict(Xs)
    sil.append(silhouette_score(Xs, labels_s))
best_k = list(k_range)[int(np.argmax(sil))]

# Final fit on full data
labels_agg = AgglomerativeClustering(n_clusters=best_k, linkage="ward").fit_predict(X)

# PCA for plotting
X_pca = PCA(n_components=2, random_state=42).fit_transform(X)

# Scatter
plt.figure(figsize=(6,5))
plt.scatter(X_pca[:,0], X_pca[:,1], c=labels_agg, s=12)
plt.title(f"PCA – Agglomerative (k={best_k})")
plt.xlabel("PC1"); plt.ylabel("PC2")
plt.tight_layout()
plt.show()

# --- Dendrogram (sampled) ---
from scipy.cluster.hierarchy import linkage, dendrogram

# Sample (keeping it small so it renders nicely)
rng = np.random.default_rng(42)
sample_n = min(300, X.shape[0])
ids = rng.choice(X.shape[0], size=sample_n, replace=False)

Z = linkage(X[ids], method="ward")
plt.figure(figsize=(10, 4))
dendrogram(Z, no_labels=True, count_sort=True)
plt.title(f"Dendrogram (Ward) – sample n={sample_n}")
plt.tight_layout()
plt.show()